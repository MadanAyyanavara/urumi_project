# System Design & Tradeoffs

## Architecture Choice
We chose a **Namespace-per-Tenant** architecture for the Store Provisioning Platform.
*   **Why:** Kubernetes Namespaces provide the strongest logical isolation for standard resources. It allows identical store configurations (Service names like `store-db`, `store-app`) to coexist without collision.
*   **Tradeoff:** Creating a namespace for every store adds some control-plane overhead to the cluster compared to a shared-namespace model, but the security and management benefits (easy teardown by deleting namespace) outweigh this for a provisioning platform.

## Orchestration Flow
1.  **Request:** User asks for a store (e.g., `my-shop`).
2.  **Validation:** Backend checks if `my-shop` already exists to prevent duplicate installs (Idempotency check).
3.  **Execution:** Backend invokes `helm install` in a child process.
4.  **Isolation:** Helm creates a new Namespace `my-shop` and deploys all resources inside it.
5.  **Peristence:** A PVC is bound to the namespace, ensuring data survives pod restarts.

## Idempotency & Failure Handling
*   **Idempotency:** The backend uses an in-memory store (or DB in prod) to track provisioning status. If a user retries creating `shop-1` while it's PROVISIONING or READY, the system detects it and returns the existing status instead of spawning duplicate Helm processes.
*   **Failure Recovery:** If `helm install` fails (e.g., insufficient cluster resources), the backend catches the error, logs the stderr, and marks the store as `FAILED` in the dashboard. The user can then delete (clean up) and retry.
*   **Cleanup:** We leverage Kubernetes cascading deletion. Deleting the Helm release (or the Namespace) automatically garbage collects all dependent resources (Deployments, Secrets, PVCs).

## Production vs. Local (Helm Strategy)
We adhere to "Config-as-Code" using Helm Values to handle environment differences.

| Feature | Local (Kind) | Production (VPS/k3s) |
| :--- | :--- | :--- |
| **Domain** | `.localtest.me` (resolves to 127.0.0.1) | A real DNS A-record pointing to VPS IP |
| **Ingress** | NGINX Ingress (NodePort/HostPort) | Traefik or NGINX (LoadBalancer) |
| **Storage** | `standard` (hostPath) | `local-path`, `gp2` (EBS), or `longhorn` |
| **Secrets** | Generated by Helm (Random/User provided) | External Secrets Operator or Sealed Secrets |
| **Resources** | Low requests (CPU: 50m) to fit laptop | High limits (CPU: 1000m+) for traffic |

## Security Posture
*   **Secrets Management:** No passwords are stored in plain text in `deployment.yaml`. They are injected via Kubernetes Secrets (`store-secrets`) and mounted as environment variables.
*   **Isolation:** `ResourceQuota` and `LimitRange` are applied per-namespace to prevent one store from consuming all cluster resources (Neighbors from Hell problem).
